[ğŸ  Accueil](../index.md) | [ğŸ“‹ Sommaire](../sommaire.md) | [ğŸ“– Lexique](lexique.md) | [â¬…ï¸ PrÃ©cÃ©dent](02-jvm-platform-threads.md) | [â¡ï¸ Suivant](04-virtual-threads-intro.md)

---

# 3. Limitations RÃ©elles des Platform Threads

## 3.1 Le problÃ¨me fondamental : Thread-per-Request

### Architecture classique d'un serveur

```
Serveur HTTP classique (Tomcat, Jetty, etc.)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Thread Pool (200 threads)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Client 1 â†’ [Thread 1] â†’ Handler â†’ Response     â”‚
â”‚  Client 2 â†’ [Thread 2] â†’ Handler â†’ Response     â”‚
â”‚  Client 3 â†’ [Thread 3] â†’ Handler â†’ Response     â”‚
â”‚  ...                                            â”‚
â”‚  Client 200 â†’ [Thread 200] â†’ Handler â†’ Response â”‚
â”‚                                                 â”‚
â”‚  âš ï¸ Client 201 â†’ [QUEUE] En attente...          â”‚
â”‚  âš ï¸ Client 202 â†’ [QUEUE] En attente...          |
â”‚  âš ï¸ Client 203 â†’ [QUEUE] En attente...          |
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ProblÃ¨me:
â€¢ 200 threads max = 200 requÃªtes simultanÃ©es max
â€¢ Au-delÃ : les requÃªtes attendent dans une queue
â€¢ Si tous les threads sont bloquÃ©s en I/O â†’ CPU Ã  5%
â€¢ Mais impossible d'accepter plus de clients!
```

### DÃ©monstration avec un serveur HTTP simple

```java
import com.sun.net.httpserver.*;
import java.io.*;
import java.net.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

public class ThreadPerRequestDemo {
    
    private static final AtomicInteger activeRequests = new AtomicInteger(0);
    private static final AtomicInteger queuedRequests = new AtomicInteger(0);
    
    public static void main(String[] args) throws IOException {
        
        // Serveur avec pool LIMITÃ‰ Ã  10 threads
        HttpServer server = HttpServer.create(new InetSocketAddress(8080), 0);
        
        // Thread pool de 10 threads seulement
        ThreadPoolExecutor executor = new ThreadPoolExecutor(
            10,  // Core pool size
            10,  // Max pool size
            60L, TimeUnit.SECONDS,
            new ArrayBlockingQueue<>(50)  // Queue de 50 max
        );
        
        server.setExecutor(executor);
        
        // Handler qui simule I/O bloquant
        server.createContext("/api/data", exchange -> {
            int active = activeRequests.incrementAndGet();
            int queued = executor.getQueue().size();
            
            String threadName = Thread.currentThread().getName();
            
            System.out.printf("[%s] RequÃªte reÃ§ue (Active: %d, Queue: %d)%n",
                threadName, active, queued);
            
            try {
                // Simulation I/O bloquant (DB, API externe, etc.)
                Thread.sleep(5000); // 5 secondes de blocage!
                
                String response = String.format(
                    "TraitÃ© par %s (Active: %d, Queue: %d)",
                    threadName, active, queued
                );
                
                exchange.sendResponseHeaders(200, response.length());
                OutputStream os = exchange.getResponseBody();
                os.write(response.getBytes());
                os.close();
                
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            } finally {
                activeRequests.decrementAndGet();
            }
        });
        
        // Endpoint de monitoring
        server.createContext("/stats", exchange -> {
            String stats = String.format(
                "Active: %d, Queue: %d, Pool size: %d",
                executor.getActiveCount(),
                executor.getQueue().size(),
                executor.getPoolSize()
            );
            
            exchange.sendResponseHeaders(200, stats.length());
            OutputStream os = exchange.getResponseBody();
            os.write(stats.getBytes());
            os.close();
        });
        
        server.start();
        System.out.println("Serveur dÃ©marrÃ© sur http://localhost:8080");
        System.out.println("Thread pool: 10 threads max");
        System.out.println("Queue: 50 requÃªtes max");
        System.out.println("\nTestez avec: curl http://localhost:8080/api/data");
    }
}

/* Test de charge (dans un autre terminal):

# Envoyer 20 requÃªtes simultanÃ©es
for i in {1..20}; do
  curl http://localhost:8080/api/data &
done

Output serveur:
[pool-1-thread-1] RequÃªte reÃ§ue (Active: 1, Queue: 0)
[pool-1-thread-2] RequÃªte reÃ§ue (Active: 2, Queue: 0)
...
[pool-1-thread-10] RequÃªte reÃ§ue (Active: 10, Queue: 0)
[pool-1-thread-8] RequÃªte reÃ§ue (Active: 10, Queue: 9)  â† En queue!
[pool-1-thread-3] RequÃªte reÃ§ue (Active: 10, Queue: 8)

Observation:
â€¢ Les 10 premiers threads traitent immÃ©diatement
â€¢ Les 10 suivants attendent dans la queue
â€¢ Latence requÃªte 11-20: +5 secondes (temps d'attente)
â€¢ Si > 60 requÃªtes: RejectedExecutionException!
*/
```

### Impact sur la latence

```
ScÃ©nario: 1000 requÃªtes/seconde avec pool de 200 threads
Temps traitement par requÃªte: 500ms (dont 450ms I/O bloquant)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Latence en fonction du nombre de clients    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚ Latence                                             â”‚
â”‚  (ms)                                               â”‚
â”‚ 5000â”‚                              ****             â”‚
â”‚     â”‚                         *****                 â”‚
â”‚ 4000â”‚                    *****                      â”‚
â”‚     â”‚               *****                           â”‚
â”‚ 3000â”‚          *****                                â”‚
â”‚     â”‚     *****                                     â”‚
â”‚ 2000â”‚ ****                                          â”‚
â”‚ 1000â”‚***                                            â”‚
â”‚  500â”‚                                               â”‚
â”‚    0â””â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€    â”‚
â”‚      0    200   400   600   800  1000  1200        â”‚
â”‚              RequÃªtes simultanÃ©es                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

InterprÃ©tation:
â€¢ 0-200 clients: Latence stable (~500ms)
â€¢ 200-400 clients: Latence double (~1000ms) - queueing
â€¢ 400+ clients: Latence explose (> 3000ms)
â€¢ > 1000 clients: Timeout ou crash
```

---

## 3.2 Le coÃ»t du Context Switching intensif

### Benchmark : Impact du nombre de threads

```java
import java.util.concurrent.*;
import java.util.List;
import java.util.ArrayList;

public class ContextSwitchingBenchmark {
    
    public static void main(String[] args) throws InterruptedException {
        
        System.out.println("=== Benchmark Context Switching ===\n");
        
        // Test avec diffÃ©rents nombres de threads
        int[] threadCounts = {10, 50, 100, 500, 1000, 5000};
        
        for (int numThreads : threadCounts) {
            benchmarkWithThreads(numThreads);
        }
    }
    
    private static void benchmarkWithThreads(int numThreads) 
            throws InterruptedException {
        
        CountDownLatch startLatch = new CountDownLatch(1);
        CountDownLatch doneLatch = new CountDownLatch(numThreads);
        
        // Compteur d'incrÃ©mentations par thread
        int incrementsPerThread = 100_000;
        
        List<Thread> threads = new ArrayList<>();
        
        // CrÃ©er les threads
        for (int i = 0; i < numThreads; i++) {
            Thread t = new Thread(() -> {
                try {
                    startLatch.await(); // Attendre le signal de dÃ©part
                    
                    // Travail CPU-bound lÃ©ger
                    long sum = 0;
                    for (int j = 0; j < incrementsPerThread; j++) {
                        sum += j;
                    }
                    
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                } finally {
                    doneLatch.countDown();
                }
            });
            t.start();
            threads.add(t);
        }
        
        // Laisser les threads se prÃ©parer
        Thread.sleep(100);
        
        // DÃ©marrer tous les threads en mÃªme temps
        long startTime = System.nanoTime();
        startLatch.countDown();
        
        // Attendre que tous terminent
        doneLatch.await();
        long duration = System.nanoTime() - startTime;
        
        // Calculs
        long totalOps = (long) numThreads * incrementsPerThread;
        double durationMs = duration / 1_000_000.0;
        double opsPerSec = totalOps / (duration / 1_000_000_000.0);
        
        System.out.printf("Threads: %5d | DurÃ©e: %7.0f ms | " +
                         "Ops/sec: %12.0f | Overhead: %s%n",
            numThreads, durationMs, opsPerSec, 
            getOverheadIndicator(numThreads)
        );
        
        // Nettoyer
        for (Thread t : threads) {
            t.join();
        }
    }
    
    private static String getOverheadIndicator(int numThreads) {
        int cores = Runtime.getRuntime().availableProcessors();
        if (numThreads <= cores) return "âœ“ Optimal";
        if (numThreads <= cores * 2) return "âš  Acceptable";
        if (numThreads <= cores * 10) return "âš âš  Ã‰levÃ©";
        return "âŒ Critique";
    }
}

/* RÃ©sultats typiques (machine 8 cores):

=== Benchmark Context Switching ===

Threads:    10 | DurÃ©e:     125 ms | Ops/sec:  8000000000 | Overhead: âœ“ Optimal
Threads:    50 | DurÃ©e:     156 ms | Ops/sec:  6410256410 | Overhead: âš  Acceptable
Threads:   100 | DurÃ©e:     234 ms | Ops/sec:  4273504274 | Overhead: âš âš  Ã‰levÃ©
Threads:   500 | DurÃ©e:    1250 ms | Ops/sec:   800000000 | Overhead: âŒ Critique
Threads:  1000 | DurÃ©e:    2890 ms | Ops/sec:   346020761 | Overhead: âŒ Critique
Threads:  5000 | DurÃ©e:   18500 ms | Ops/sec:    54054054 | Overhead: âŒ Critique

Observations:
â€¢ 10 threads (â‰ˆ nb cores): Performance optimale
â€¢ 100 threads: Performance divisÃ©e par 2 (context switching)
â€¢ 1000 threads: Performance divisÃ©e par 7 !
â€¢ 5000 threads: Performance divisÃ©e par 25 !!

Le CPU passe plus de temps Ã  switcher qu'Ã  travailler!
*/
```

### Visualisation du CPU usage

```
CPU Utilization avec 10 threads (optimal):
Core 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  95%
Core 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  95%
Core 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  95%
Core 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  95%
Context switching: ~5% (nÃ©gligeable)

CPU Utilization avec 1000 threads (critique):
Core 1: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%
Core 2: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%
Core 3: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%
Core 4: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%
Context switching: ~70% (catastrophique!)

LÃ©gende:
â–ˆ = Travail utile
â–‘ = Context switching / scheduling overhead
```

---

## 3.3 Limitation mÃ©moire : Le mur des 10,000 threads

### Calcul thÃ©orique

```
Calcul simple:

Machine: 16 GB RAM

MÃ©moire par thread:
â€¢ Stack size: 1-2 MB (configurable avec -Xss)
â€¢ TCB (Thread Control Block): ~1 KB
â€¢ MÃ©tadonnÃ©es JVM: ~1 KB
â€¢ Total: ~2 MB par thread

Nombre max thÃ©orique:
â€¢ 16 GB / 2 MB = 8,000 threads

Mais en rÃ©alitÃ©:
â€¢ JVM Heap: 4-8 GB
â€¢ Code (classes): ~500 MB
â€¢ Metaspace: ~500 MB
â€¢ OS kernel: ~1 GB
â€¢ Buffers rÃ©seau: ~500 MB
â€¢ Direct memory: ~1 GB

MÃ©moire disponible pour threads: ~8 GB
â€¢ 8 GB / 2 MB = 4,000 threads max pratique

Au-delÃ : OutOfMemoryError ou thrashing
```

### DÃ©monstration pratique

```java
import java.util.ArrayList;
import java.util.List;

public class ThreadMemoryLimit {
    
    public static void main(String[] args) {
        
        List<Thread> threads = new ArrayList<>();
        int count = 0;
        
        System.out.println("CrÃ©ation de threads jusqu'Ã  l'erreur...");
        System.out.println("Stack size: " + 
            Thread.currentThread().getStackSize() / 1024 + " KB");
        
        Runtime runtime = Runtime.getRuntime();
        
        try {
            while (true) {
                Thread t = new Thread(() -> {
                    try {
                        // Dormir indÃ©finiment
                        Thread.sleep(Long.MAX_VALUE);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                    }
                });
                
                t.start();
                threads.add(t);
                count++;
                
                // Afficher tous les 1000 threads
                if (count % 1000 == 0) {
                    long usedMemory = runtime.totalMemory() - runtime.freeMemory();
                    System.out.printf("Threads crÃ©Ã©s: %5d | MÃ©moire: %5d MB%n",
                        count, usedMemory / 1024 / 1024);
                }
            }
        } catch (OutOfMemoryError e) {
            System.out.println("\nâŒ OutOfMemoryError atteint!");
            System.out.println("Nombre max de threads: " + count);
            
            long usedMemory = runtime.totalMemory() - runtime.freeMemory();
            System.out.println("MÃ©moire utilisÃ©e: " + usedMemory / 1024 / 1024 + " MB");
            System.out.println("MÃ©moire par thread: " + 
                (usedMemory / count / 1024) + " KB");
            
        } finally {
            // Nettoyer
            System.out.println("\nInterruption des threads...");
            threads.forEach(Thread::interrupt);
        }
    }
}

/* Output typique (machine 16 GB, JVM avec -Xmx8g):

CrÃ©ation de threads jusqu'Ã  l'erreur...
Stack size: 1024 KB
Threads crÃ©Ã©s:  1000 | MÃ©moire:  2100 MB
Threads crÃ©Ã©s:  2000 | MÃ©moire:  4200 MB
Threads crÃ©Ã©s:  3000 | MÃ©moire:  6300 MB
Threads crÃ©Ã©s:  4000 | MÃ©moire:  8400 MB
Threads crÃ©Ã©s:  5000 | MÃ©moire: 10500 MB

âŒ OutOfMemoryError atteint!
Nombre max de threads: 5247
MÃ©moire utilisÃ©e: 11000 MB
MÃ©moire par thread: 2145 KB

Conclusion:
â€¢ Machine 16 GB â†’ Max ~5000 threads
â€¢ Au-delÃ : Impossible d'allouer plus de mÃ©moire
â€¢ MÃªme si CPU idle Ã  90%!
*/
```

### Impact sur les architectures modernes

```
Application Microservice moderne:

Besoins rÃ©els:
â€¢ API Gateway: 10,000 connexions simultanÃ©es
â€¢ User Service: 5,000 connexions
â€¢ Order Service: 3,000 connexions
â€¢ Payment Service: 2,000 connexions
Total: 20,000 connexions simultanÃ©es

Avec Platform Threads:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway                            â”‚
â”‚ â€¢ 200 threads â†’ 200 connexions max     â”‚
â”‚ â€¢ Les 9,800 autres en QUEUE âŒ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Service                           â”‚
â”‚ â€¢ 200 threads â†’ 200 connexions max     â”‚
â”‚ â€¢ Les 4,800 autres en QUEUE âŒ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Solution actuelle (coÃ»teuse):
â€¢ Horizontal Scaling: 50 instances pour Gateway
â€¢ 25 instances pour User Service
â€¢ CoÃ»t cloud: $$$$$
â€¢ ComplexitÃ©: Load balancing, orchestration
```

---

## 3.4 I/O Bloquant : Le grand gaspillage

### Analyse d'une application rÃ©elle

```java
import java.sql.*;
import java.net.URI;
import java.net.http.*;
import java.time.Duration;

public class RealWorldScenario {
    
    private static final HttpClient httpClient = HttpClient.newHttpClient();
    
    public static void main(String[] args) throws Exception {
        
        System.out.println("=== Simulation application rÃ©elle ===\n");
        
        // Profiler une requÃªte typique
        profileRequest();
    }
    
    private static void profileRequest() throws Exception {
        
        long totalStart = System.nanoTime();
        long lastCheckpoint = totalStart;
        
        System.out.println("[Phase 1] Validation paramÃ¨tres");
        Thread.sleep(5); // Simulation
        printDuration("Validation", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 2] RequÃªte base de donnÃ©es");
        // Simulation JDBC
        Thread.sleep(50);
        printDuration("DB Query", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 3] Appel API externe (auth)");
        // Simulation HTTP
        Thread.sleep(120);
        printDuration("Auth API", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 4] Traitement business logic");
        Thread.sleep(10);
        printDuration("Business Logic", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 5] RequÃªte cache (Redis)");
        Thread.sleep(15);
        printDuration("Cache", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 6] Appel API externe (notification)");
        Thread.sleep(80);
        printDuration("Notification API", lastCheckpoint);
        lastCheckpoint = System.nanoTime();
        
        System.out.println("\n[Phase 7] SÃ©rialisation rÃ©ponse");
        Thread.sleep(5);
        printDuration("Serialization", lastCheckpoint);
        
        long totalDuration = System.nanoTime() - totalStart;
        
        System.out.println("\n" + "=".repeat(50));
        System.out.println("DURÃ‰E TOTALE: " + totalDuration / 1_000_000 + " ms");
        System.out.println("=".repeat(50));
        
        // Analyse
        analyzeEfficiency();
    }
    
    private static void printDuration(String phase, long lastCheckpoint) {
        long duration = (System.nanoTime() - lastCheckpoint) / 1_000_000;
        System.out.printf("  âœ“ %s: %d ms%n", phase, duration);
    }
    
    private static void analyzeEfficiency() {
        System.out.println("\nAnalyse d'efficacitÃ©:");
        System.out.println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        System.out.println("â”‚ Phase                   â”‚ DurÃ©e    â”‚ Type       â”‚");
        System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        System.out.println("â”‚ Validation              â”‚    5 ms  â”‚ CPU âœ“      â”‚");
        System.out.println("â”‚ DB Query                â”‚   50 ms  â”‚ I/O âš       â”‚");
        System.out.println("â”‚ Auth API                â”‚  120 ms  â”‚ I/O âš       â”‚");
        System.out.println("â”‚ Business Logic          â”‚   10 ms  â”‚ CPU âœ“      â”‚");
        System.out.println("â”‚ Cache                   â”‚   15 ms  â”‚ I/O âš       â”‚");
        System.out.println("â”‚ Notification API        â”‚   80 ms  â”‚ I/O âš       â”‚");
        System.out.println("â”‚ Serialization           â”‚    5 ms  â”‚ CPU âœ“      â”‚");
        System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        System.out.println("â”‚ TOTAL                   â”‚  285 ms  â”‚            â”‚");
        System.out.println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        
        System.out.println("\nğŸ“Š RÃ©partition:");
        System.out.println("  â€¢ Temps CPU actif:  20 ms (  7%)  â† Travail utile");
        System.out.println("  â€¢ Temps I/O bloquÃ©: 265 ms ( 93%)  â† GASPILLAGE!");
        
        System.out.println("\nâš ï¸  ProblÃ¨me:");
        System.out.println("  Le thread Platform reste BLOQUÃ‰ pendant 265ms");
        System.out.println("  â†’ 2 MB de mÃ©moire inutilisÃ©s");
        System.out.println("  â†’ 1 slot du thread pool monopolisÃ©");
        System.out.println("  â†’ CPU idle");
        
        System.out.println("\nğŸ’¡ Avec 200 threads Platform:");
        System.out.println("  â€¢ Si 200 requÃªtes arrivent simultanÃ©ment");
        System.out.println("  â€¢ 200 threads tous BLOQUÃ‰S 93% du temps");
        System.out.println("  â€¢ CPU utilization: ~7%");
        System.out.println("  â€¢ Mais impossible d'accepter plus de requÃªtes!");
    }
}

/* Output:

=== Simulation application rÃ©elle ===

[Phase 1] Validation paramÃ¨tres
  âœ“ Validation: 5 ms

[Phase 2] RequÃªte base de donnÃ©es
  âœ“ DB Query: 50 ms

[Phase 3] Appel API externe (auth)
  âœ“ Auth API: 120 ms

[Phase 4] Traitement business logic
  âœ“ Business Logic: 10 ms

[Phase 5] RequÃªte cache (Redis)
  âœ“ Cache: 15 ms

[Phase 6] Appel API externe (notification)
  âœ“ Notification API: 80 ms

[Phase 7] SÃ©rialisation rÃ©ponse
  âœ“ Serialization: 5 ms

==================================================
DURÃ‰E TOTALE: 285 ms
==================================================

Analyse d'efficacitÃ©:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase                   â”‚ DurÃ©e    â”‚ Type       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Validation              â”‚    5 ms  â”‚ CPU âœ“      â”‚
â”‚ DB Query                â”‚   50 ms  â”‚ I/O âš       â”‚
â”‚ Auth API                â”‚  120 ms  â”‚ I/O âš       â”‚
â”‚ Business Logic          â”‚   10 ms  â”‚ CPU âœ“      â”‚
â”‚ Cache                   â”‚   15 ms  â”‚ I/O âš       â”‚
â”‚ Notification API        â”‚   80 ms  â”‚ I/O âš       â”‚
â”‚ Serialization           â”‚    5 ms  â”‚ CPU âœ“      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                   â”‚  285 ms  â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š RÃ©partition:
  â€¢ Temps CPU actif:  20 ms (  7%)  â† Travail utile
  â€¢ Temps I/O bloquÃ©: 265 ms ( 93%)  â† GASPILLAGE!

âš ï¸  ProblÃ¨me:
  Le thread Platform reste BLOQUÃ‰ pendant 265ms
  â†’ 2 MB de mÃ©moire inutilisÃ©s
  â†’ 1 slot du thread pool monopolisÃ©
  â†’ CPU idle

ğŸ’¡ Avec 200 threads Platform:
  â€¢ Si 200 requÃªtes arrivent simultanÃ©ment
  â€¢ 200 threads tous BLOQUÃ‰S 93% du temps
  â€¢ CPU utilization: ~7%
  â€¢ Mais impossible d'accepter plus de requÃªtes!
*/
```

### Timeline visuelle d'une requÃªte

```
Thread Platform - Timeline (285ms total):

0ms        50ms       170ms    180ms 195ms     275ms 285ms
â”‚          â”‚          â”‚        â”‚     â”‚         â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚          â”‚          â”‚        â”‚     â”‚         â”‚     â”‚
CPU  I/O   CPU  I/O   CPU I/O  CPU   I/O   CPU I/O  CPU
 5ms 50ms  120ms 10ms 15ms     80ms  5ms

DÃ©tail:
â–ˆ = CPU actif (20ms total = 7%)
â–‘ = I/O bloquÃ© (265ms total = 93%)

â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ

Le thread passe 93% de son temps Ã  NE RIEN FAIRE!
```

---

## 3.5 Cas d'usage critiques

### 3.5.1 WebSocket et Connexions longues

```java
import javax.websocket.*;
import javax.websocket.server.ServerEndpoint;
import java.util.Set;
import java.util.concurrent.CopyOnWriteArraySet;

@ServerEndpoint("/chat")
public class ChatWebSocket {
    
    private static final Set<Session> sessions = new CopyOnWriteArraySet<>();
    
    @OnOpen
    public void onOpen(Session session) {
        sessions.add(session);
        System.out.println("Nouvelle connexion: " + session.getId());
        System.out.println("Connexions actives: " + sessions.size());
        
        // âš ï¸ PROBLÃˆME CRITIQUE:
        // Chaque connexion WebSocket = 1 thread Platform BLOQUÃ‰
        // Le thread reste ouvert tant que la connexion est active
        // Peut Ãªtre des heures ou des jours!
        
        // Avec 200 threads max:
        // 200 connexions WebSocket = thread pool SATURÃ‰
        // â†’ Plus aucune requÃªte HTTP ne peut Ãªtre traitÃ©e!
    }
    
    @OnMessage
    public void onMessage(String message, Session session) {
        // Broadcaster Ã  tous
        sessions.forEach(s -> {
            try {
                s.getBasicRemote().sendText(message);
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
    }
    
    @OnClose
    public void onClose(Session session) {
        sessions.remove(session);
        System.out.println("Connexion fermÃ©e: " + session.getId());
        System.out.println("Connexions restantes: " + sessions.size());
    }
}

/*
ScÃ©nario catastrophe:

1. Serveur avec 200 threads Platform
2. 150 clients WebSocket se connectent
3. â†’ 150 threads Platform BLOQUÃ‰S indÃ©finiment
4. â†’ Il reste 50 threads pour les requÃªtes HTTP
5. â†’ Si 50+ requÃªtes HTTP arrivent â†’ QUEUE
6. â†’ Latence explose pour les requÃªtes normales

Solution actuelle: Serveur dÃ©diÃ© pour WebSocket
CoÃ»t: Infrastructure additionnelle, complexitÃ©
*/
```

### 3.5.2 Traitement Batch massivement parallÃ¨le

```java
import java.util.List;
import java.util.ArrayList;
import java.util.concurrent.*;

public class BatchProcessingProblem {
    
    public static void main(String[] args) throws Exception {
        
        // ScÃ©nario: Traiter 100,000 enregistrements
        List<Integer> records = new ArrayList<>();
        for (int i = 0; i < 100_000; i++) {
            records.add(i);
        }
        
        System.out.println("=== Traitement avec Platform Threads ===\n");
        
        // Approche 1: Thread pool fixe (50 threads)
        System.out.println("Approche 1: Pool fixe de 50 threads");
        long start = System.currentTimeMillis();
        
        ExecutorService executor = Executors.newFixedThreadPool(50);
        List<Future<String>> futures = new ArrayList<>();
        
        for (Integer record : records) {
            futures.add(executor.submit(() -> processRecord(record)));
        }
        
        // Attendre tous les rÃ©sultats
        for (Future<String> future : futures) {
            future.get();
        }
        
        executor.shutdown();
        long duration1 = System.currentTimeMillis() - start;
        
        System.out.println("Temps: " + duration1 + " ms");
        System.out.println("Throughput: " + (100_000.0 / duration1 * 1000) + " records/sec");
        
        // Approche 2: Pool plus grand (500 threads)
        System.out.println("\nApproche 2: Pool de 500 threads");
        start = System.currentTimeMillis();
        
        executor = Executors.newFixedThreadPool(500);
        futures = new ArrayList<>();
        
        for (Integer record : records) {
            futures.add(executor.submit(() -> processRecord(record)));
        }
        
        for (Future<String> future : futures) {
            future.get();
        }
        
        executor.shutdown();
        long duration2 = System.currentTimeMillis() - start;
        
        System.out.println("Temps: " + duration2 + " ms");
        System.out.println("Throughput: " + (100_000.0 / duration2 * 1000) + " records/sec");
        
        // Analyse
        System.out.println("\n=== Analyse ===");
        System.out.println("50 threads:  " + duration1 + " ms");
        System.out.println("500 threads: " + duration2 + " ms");
        
        if (duration2 < duration1) {
            double improvement = ((duration1 - duration2) / (double) duration1) * 100;
            System.out.println("AmÃ©lioration: " + String.format("%.1f", improvement) + "%");
        } else {
            System.out.println("âš ï¸  Plus de threads = PLUS LENT!");
            System.out.println("Raison: Context switching overhead > gain parallÃ©lisme");
        }
        
        System.out.println("\nâš ï¸  ProblÃ¨me:");
        System.out.println("â€¢ 500 threads = 1 GB de mÃ©moire (stack)");
        System.out.println("â€¢ Context switching intensif");
        System.out.println("â€¢ Pas scalable Ã  1 million de records");
    }
    
    private static String processRecord(int recordId) {
        try {
            // Simulation: I/O bloquant (DB write, API call)
            Thread.sleep(100);
            return "Processed: " + recordId;
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return "Error: " + recordId;
        }
    }
}

/* Output typique:

=== Traitement avec Platform Threads ===

Approche 1: Pool fixe de 50 threads
Temps: 200800 ms (â‰ˆ 3min 20s)
Throughput: 498.0 records/sec

Approche 2: Pool de 500 threads
Temps: 21200 ms (â‰ˆ 21s)
Throughput: 4716.9 records/sec

=== Analyse ===
50 threads:  200800 ms
500 threads: 21200 ms
AmÃ©lioration: 89.4%

âš ï¸  ProblÃ¨me:
â€¢ 500 threads = 1 GB de mÃ©moire (stack)
â€¢ Context switching intensif
â€¢ Pas scalable Ã  1 million de records

Avec 1 million de records:
â€¢ 500 threads â†’ 200+ secondes
â€¢ Pour aller plus vite â†’ plus de threads
â€¢ Mais 5000 threads = OutOfMemoryError!
*/
```

### 3.5.3 Microservices avec appels en cascade

```java
import java.net.http.*;
import java.net.URI;
import java.util.concurrent.*;

public class MicroserviceCascade {
    
    private static final HttpClient httpClient = HttpClient.newHttpClient();
    
    public static void main(String[] args) throws Exception {
        
        System.out.println("=== Simulation cascade microservices ===\n");
        
        // Service A appelle Service B qui appelle Service C
        String result = serviceA();
        System.out.println("RÃ©sultat: " + result);
    }
    
    // Service A: API Gateway
    private static String serviceA() throws Exception {
        System.out.println("[Service A] RequÃªte reÃ§ue");
        long start = System.nanoTime();
        
        // Appel Service B (bloque le thread)
        String resultB = serviceB();
        
        // Traitement
        Thread.sleep(10);
        
        long duration = (System.nanoTime() - start) / 1_000_000;
        System.out.println("[Service A] TerminÃ© en " + duration + "ms");
        
        return "A(" + resultB + ")";
    }
    
    // Service B: User Service
    private static String serviceB() throws Exception {
        System.out.println("  [Service B] RequÃªte reÃ§ue");
        long start = System.nanoTime();
        
        // Appel Service C (bloque le thread)
        String resultC = serviceC();
        
        // Traitement
        Thread.sleep(15);
        
        long duration = (System.nanoTime() - start) / 1_000_000;
        System.out.println("  [Service B] TerminÃ© en " + duration + "ms");
        
        return "B(" + resultC + ")";
    }
    
    // Service C: Database Service
    private static String serviceC() throws Exception {
        System.out.println("    [Service C] RequÃªte reÃ§ue");
        long start = System.nanoTime();
        
        // RequÃªte DB (bloque le thread)
        Thread.sleep(80);
        
        long duration = (System.nanoTime() - start) / 1_000_000;
        System.out.println("    [Service C] TerminÃ© en " + duration + "ms");
        
        return "C(data)";
    }
}

/* Output:

=== Simulation cascade microservices ===

[Service A] RequÃªte reÃ§ue
  [Service B] RequÃªte reÃ§ue
    [Service C] RequÃªte reÃ§ue
    [Service C] TerminÃ© en 80ms
  [Service B] TerminÃ© en 95ms
[Service A] TerminÃ© en 105ms
RÃ©sultat: A(B(C(data)))

Analyse du problÃ¨me:

Timeline des threads:

Service A thread: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ]
                   â–²                                  â–²
                   Actif                           Actif
                   â”‚â† BloquÃ© en attente Service B â†’â”‚

Service B thread: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ]
                   â–²                            â–²
                   Actif                     Actif
                   â”‚â† BloquÃ© attente Service C â†’â”‚

Service C thread: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ]
                   â–²                        â–²
                   Actif                 Actif
                   â”‚â† BloquÃ© sur DB â†’â”‚

ProblÃ¨me:
â€¢ 3 threads Platform mobilisÃ©s pour 1 requÃªte
â€¢ Chaque thread bloquÃ© 90% du temps
â€¢ Avec 1000 requÃªtes simultanÃ©es â†’ 3000 threads!
â€¢ OutOfMemoryError garanti

Impact production:
Service A: 200 threads â†’ max 200 requÃªtes/sec
Service B: 200 threads â†’ max 200 requÃªtes/sec
Service C: 200 threads â†’ max 200 requÃªtes/sec

Si Service C est lent (spike de latence):
â†’ Tous les threads de A et B se bloquent
â†’ Effet domino sur toute l'architecture
â†’ Timeout en cascade
*/
```

---

## 3.6 Les "Solutions" actuelles et leurs limites

### 3.6.1 Augmenter le nombre de threads

```java
// Configuration Tomcat typique
server.tomcat.threads.max=1000  // Au lieu de 200

/* ProblÃ¨mes:
âŒ Consommation mÃ©moire: 1000 Ã— 2MB = 2GB juste pour les stacks
âŒ Context switching: Performance dÃ©gradÃ©e
âŒ Toujours une limite: 1000 threads = 1000 connexions max
âŒ Si 2000 requÃªtes simultanÃ©es â†’ toujours un problÃ¨me!
âŒ Ne scale pas indÃ©finiment (OutOfMemoryError)

RÃ©sultat:
â€¢ CoÃ»t mÃ©moire Ã©levÃ©
â€¢ Performance dÃ©gradÃ©e (context switching)
â€¢ ProblÃ¨me non rÃ©solu, juste retardÃ©
*/
```

### 3.6.2 Programmation RÃ©active (WebFlux)

```java
// Avec Reactor (WebFlux)
public Mono<OrderDetails> getOrderDetails(Long orderId) {
    return orderRepository.findById(orderId)
        .flatMap(order -> 
            Mono.zip(
                userService.getUser(order.getUserId()),
                paymentService.getPayment(orderId),
                inventoryService.checkStock(orderId)
            ).map(tuple -> new OrderDetails(
                order,
                tuple.getT1(),
                tuple.getT2(),
                tuple.getT3()
            ))
        );
}

/* Avantages:
âœ… Non-bloquant: excellent throughput
âœ… Peu de threads nÃ©cessaires
âœ… Scale trÃ¨s bien

InconvÃ©nients:
âŒ Courbe d'apprentissage TRÃˆS raide
âŒ Debugging cauchemardesque:
   Stack traces incomprÃ©hensibles avec 50 niveaux
âŒ "Viral": tout le code doit devenir rÃ©actif
   Si une lib n'est pas rÃ©active â†’ bloque tout
âŒ Code complexe pour des opÃ©rations simples
âŒ NÃ©cessite drivers rÃ©actifs (R2DBC vs JDBC)
âŒ Beaucoup de libs Java ne supportent pas le rÃ©actif
âŒ Gestion d'erreur complexe

Exemple de stack trace rÃ©actif (illisible):
reactor.core.publisher.Mono.lambda$flatMap$1(Mono.java:...)
reactor.core.publisher.FluxFlatMap$FlatMapMain.onNext(FluxFlatMap.java:...)
reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(...)
... 47 lignes plus tard ...
Caused by: java.sql.SQLException: Connection timeout
*/
```

### 3.6.3 Async/CompletableFuture

```java
// Avec CompletableFuture
public CompletableFuture<OrderDetails> getOrderDetails(Long orderId) {
    return orderRepository.findByIdAsync(orderId)
        .thenCompose(order -> 
            CompletableFuture.allOf(
                userService.getUserAsync(order.getUserId()),
                paymentService.getPaymentAsync(orderId)
            ).thenApply(v -> new OrderDetails(order, /*...*/ ))
        );
}

/* Avantages:
âœ… Non-bloquant
âœ… API standard Java
âœ… Meilleur que les threads bloquants

InconvÃ©nients:
âŒ Code verbeux et complexe
âŒ Gestion erreur difficile (exceptionally, handle)
âŒ Composition compliquÃ©e (thenCompose, thenCombine, etc.)
âŒ Debug difficile
âŒ Toujours limitÃ© par le thread pool sous-jacent
âŒ "Viral": propage la complexitÃ©

Exemple de code pour 3 appels parallÃ¨les + 1 sÃ©quentiel:
CompletableFuture.supplyAsync(() -> getUser())
    .thenCompose(user -> 
        CompletableFuture.allOf(
            CompletableFuture.supplyAsync(() -> getOrders(user)),
            CompletableFuture.supplyAsync(() -> getPayments(user)),
            CompletableFuture.supplyAsync(() -> getPreferences(user))
        ).thenApply(v -> combine(user, ...))
    ).exceptionally(ex -> handleError(ex))
     .thenAccept(result -> sendResponse(result));

Compare avec du code synchrone simple:
User user = getUser();
Orders orders = getOrders(user);
Payments payments = getPayments(user);
Preferences prefs = getPreferences(user);
Result result = combine(user, orders, payments, prefs);
sendResponse(result);

10Ã— plus simple Ã  lire et maintenir!
*/
```

### 3.6.4 Horizontal Scaling (plus de serveurs)

```
Solution actuelle en production:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer                            â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â–º Instance 1 (200 threads)
      â”œâ”€â–º Instance 2 (200 threads)
      â”œâ”€â–º Instance 3 (200 threads)
      â”œâ”€â–º Instance 4 (200 threads)
      â”œâ”€â–º Instance 5 (200 threads)
      â”œâ”€â–º ... (50 instances)
      â””â”€â–º Instance 50 (200 threads)

Total: 10,000 threads "effectifs"
CoÃ»t: 50 instances EC2 m5.xlarge
Prix: ~$3,500/mois sur AWS

/* ProblÃ¨mes:
âŒ CoÃ»t Ã©levÃ©: $42,000/an
âŒ ComplexitÃ©: Load balancing, orchestration, monitoring
âŒ Latence: Session affinity, cross-instance calls
âŒ Waste: Chaque instance sous-utilisÃ©e (CPU Ã  10%)
âŒ Management: DÃ©ploiements, mises Ã  jour, scaling

Alternative avec Virtual Threads:
âœ… 5 instances seulement (mÃªme capacitÃ©)
âœ… CoÃ»t: ~$350/mois ($4,200/an)
âœ… Ã‰conomie: $37,800/an (90% de rÃ©duction!)
âœ… ComplexitÃ© rÃ©duite
âœ… Meilleure utilisation CPU (70-80%)
*/
```

---

## 3.7 Tableau rÃ©capitulatif des limitations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Limitations des Platform Threads                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Limitation          â”‚ Impact                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MÃ©moire             â”‚ â€¢ 2 MB par thread                         â”‚
â”‚                     â”‚ â€¢ Max ~5000 threads (16GB RAM)            â”‚
â”‚                     â”‚ â€¢ OutOfMemoryError au-delÃ                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Context Switching   â”‚ â€¢ Overhead 10-70% avec 1000+ threads      â”‚
â”‚                     â”‚ â€¢ CPU time gaspillÃ©                       â”‚
â”‚                     â”‚ â€¢ Cache thrashing                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CrÃ©ation            â”‚ â€¢ 0.5-1 ms par thread                     â”‚
â”‚                     â”‚ â€¢ CoÃ»teux pour short-lived tasks          â”‚
â”‚                     â”‚ â€¢ Thread pools requis                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Blocking I/O        â”‚ â€¢ 70-95% du temps en attente              â”‚
â”‚                     â”‚ â€¢ Thread inutilisÃ© mais consomme ressourcesâ”‚
â”‚                     â”‚ â€¢ Throughput limitÃ©                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ScalabilitÃ©         â”‚ â€¢ Thread-per-request â†’ max connections   â”‚
â”‚                     â”‚ â€¢ WebSocket â†’ thread pool saturation      â”‚
â”‚                     â”‚ â€¢ Batch processing limitÃ©                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CoÃ»t Cloud          â”‚ â€¢ Horizontal scaling coÃ»teux              â”‚
â”‚                     â”‚ â€¢ Sous-utilisation CPU (10-20%)           â”‚
â”‚                     â”‚ â€¢ Gaspillage ressources                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison des "solutions"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Solution         â”‚ComplexitÃ©â”‚Performanceâ”‚ScalabilitÃ©â”‚CoÃ»t Maint.â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Plus de threads  â”‚    â­    â”‚    â­â­   â”‚    â­    â”‚    â­â­    â”‚
â”‚ (1000+ threads)  â”‚          â”‚          â”‚          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Thread Pools     â”‚   â­â­   â”‚   â­â­â­  â”‚   â­â­   â”‚   â­â­â­   â”‚
â”‚ (fixed size)     â”‚          â”‚          â”‚          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CompletableFutureâ”‚  â­â­â­   â”‚  â­â­â­â­  â”‚  â­â­â­   â”‚   â­â­    â”‚
â”‚ (async)          â”‚          â”‚          â”‚          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reactive (WebFluxâ”‚ â­â­â­â­â­  â”‚ â­â­â­â­â­  â”‚ â­â­â­â­â­  â”‚    â­     â”‚
â”‚ /Reactor)        â”‚          â”‚          â”‚          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Horizontal       â”‚  â­â­â­â­  â”‚  â­â­â­â­  â”‚ â­â­â­â­â­  â”‚   â­â­    â”‚
â”‚ Scaling          â”‚          â”‚          â”‚          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Virtual Threads  â”‚    â­    â”‚ â­â­â­â­â­  â”‚ â­â­â­â­â­  â”‚  â­â­â­â­â­  â”‚
â”‚ (Java 21)        â”‚          â”‚          â”‚          â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â­ = Faible/Mauvais
â­â­â­â­â­ = Excellent

Virtual Threads = Le meilleur des deux mondes:
â€¢ SimplicitÃ© du code synchrone
â€¢ Performance du code asynchrone
â€¢ Sans les inconvÃ©nients!
```

---

## 3.8 MÃ©triques rÃ©elles : Avant Virtual Threads

### Application E-commerce typique

```
Profil d'une application e-commerce classique:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©triques Production (Platform Threads)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚ Infrastructure:                                â”‚
â”‚ â€¢ 20 instances EC2 m5.xlarge (4 vCPU, 16GB)   â”‚
â”‚ â€¢ Thread pool: 200 threads par instance        â”‚
â”‚ â€¢ Total: 4,000 threads "effectifs"             â”‚
â”‚                                                â”‚
â”‚ Performance:                                   â”‚
â”‚ â€¢ Peak load: 500 req/sec                       â”‚
â”‚ â€¢ Latence p50: 250ms                           â”‚
â”‚ â€¢ Latence p99: 1,200ms                         â”‚
â”‚ â€¢ CPU utilization: 15-25%                      â”‚
â”‚ â€¢ Memory usage: 60%                            â”‚
â”‚                                                â”‚
â”‚ CoÃ»ts mensuels:                                â”‚
â”‚ â€¢ EC2: 20 Ã— $70 = $1,400/mois                  â”‚
â”‚ â€¢ Load Balancer: $100/mois                     â”‚
â”‚ â€¢ Total: $1,500/mois                           â”‚
â”‚                                                â”‚
â”‚ ProblÃ¨mes:                                     â”‚
â”‚ â€¢ Sous-utilisation CPU massive (75% idle)      â”‚
â”‚ â€¢ Latence p99 Ã©levÃ©e (queueing)                â”‚
â”‚ â€¢ CoÃ»ts Ã©levÃ©s pour la capacitÃ© fournie        â”‚
â”‚ â€¢ Impossible de gÃ©rer plus de 500 req/sec      â”‚
â”‚   sans ajouter des instances                   â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Analyse d'une journÃ©e type

```
Load profile sur 24h:

RequÃªtes/sec
    1000â”‚                    â•­â”€â•®
        â”‚                  â•­â”€â•¯ â•°â”€â•®
     500â”‚       â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â•°â”€â”€â”€â”€â•®
        â”‚     â•­â”€â•¯                     â•°â”€â•®
     250â”‚  â•­â”€â”€â•¯                         â•°â”€â”€â•®
        â”‚â•­â”€â•¯                               â•°â”€â•®
       0â””â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€
        0h  4h  8h  12h 16h 20h 24h

Observations:
â€¢ Peak: 16h-18h (rentrÃ©e bureau) â†’ 600 req/sec
â€¢ Avec 4,000 threads â†’ proche saturation
â€¢ Night: 0h-6h â†’ 50 req/sec
â€¢ 3,900 threads INUTILISÃ‰S mais coÃ»tent!

Avec Virtual Threads:
â€¢ MÃªme infrastructure
â€¢ Peut gÃ©rer 5,000+ req/sec
â€¢ CoÃ»t identique
â€¢ Ou: diviser par 10 le nombre d'instances
  â†’ $150/mois au lieu de $1,500/mois
```

---

## 3.9 RÃ©sumÃ© : Le mur des Platform Threads

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Le triangle impossible des Platform Threads  â”‚
â”‚                                                 â”‚
â”‚                  ScalabilitÃ©                    â”‚
â”‚                      â–³                          â”‚
â”‚                     â•± â•²                         â”‚
â”‚                    â•±   â•²                        â”‚
â”‚                   â•±     â•²                       â”‚
â”‚                  â•±       â•²                      â”‚
â”‚                 â•±    âŒ    â•²                     â”‚
â”‚                â•±  Impossible â•²                  â”‚
â”‚               â•±   d'avoir les â•²                 â”‚
â”‚              â•±       trois     â•²                â”‚
â”‚             â•±                   â•²               â”‚
â”‚            â–³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–³              â”‚
â”‚      SimplicitÃ©              Performance        â”‚
â”‚                                                 â”‚
â”‚  Avec Platform Threads, choisissez 2 sur 3:    â”‚
â”‚  â€¢ Simple + Performant = Pas scalable           â”‚
â”‚  â€¢ Scalable + Performant = Complexe (reactive)  â”‚
â”‚  â€¢ Simple + Scalable = Pas performant           â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les chiffres qui font mal

```
Limites dures des Platform Threads:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©trique                 â”‚ Limite Platform  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Threads par machine      â”‚ ~5,000 max       â”‚
â”‚ Connexions simultanÃ©es   â”‚ ~5,000 max       â”‚
â”‚ MÃ©moire par thread       â”‚ 2 MB             â”‚
â”‚ Context switch overhead  â”‚ 10-70% (>1000)   â”‚
â”‚ Temps crÃ©ation           â”‚ 0.5 ms           â”‚
â”‚ EfficacitÃ© I/O bloquant  â”‚ 5-30% CPU usage  â”‚
â”‚ CoÃ»t horizontal scaling  â”‚ $$             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ce que Ã§a signifie en production:
âŒ Impossible de gÃ©rer 100,000 connexions WebSocket
âŒ Batch de 1M records â†’ 30+ minutes ou OOM
âŒ Microservices â†’ cascade de blocages
âŒ CPU idle Ã  90% mais "pas de capacitÃ©"
âŒ CoÃ»ts cloud Ã— 10 pour compenser

La conclusion est claire:
Le modÃ¨le Platform Thread ne peut PAS scale
pour les applications modernes.

â¡ï¸  Solution: Virtual Threads (chapitre suivant)
```

### Points clÃ©s Ã  retenir

```
âœ… Ce qu'il faut retenir sur les limitations:

1. Thread-per-request ne scale pas
   â€¢ Max 5,000 connexions simultanÃ©es
   â€¢ Au-delÃ : queue ou crash
   
2. I/O bloquant = gaspillage massif
   â€¢ 70-95% du temps en attente
   â€¢ CPU idle mais capacitÃ© saturÃ©e
   
3. Context switching = overhead critique
   â€¢ >1,000 threads â†’ 50% overhead
   â€¢ Performance s'effondre
   
4. MÃ©moire = limite dure
   â€¢ 2 MB par thread
   â€¢ OutOfMemoryError inÃ©vitable
   
5. Les "solutions" actuelles ont des compromis
   â€¢ Reactive: complexe
   â€¢ Async: verbeux
   â€¢ Scaling: coÃ»teux
   
6. Le problÃ¨me est fondamental
   â€¢ Pas un bug Ã  fixer
   â€¢ Mais une limite architecturale
   â€¢ NÃ©cessite une nouvelle approche

ğŸ¯ Virtual Threads changent la donne:
   â€¢ Millions de threads possibles
   â€¢ Code simple (synchrone)
   â€¢ Performance excellente
   â€¢ Sans les compromis!
```

---

[ğŸ  Accueil](../index.md) | [ğŸ“‹ Sommaire](../sommaire.md) | [â¬…ï¸ PrÃ©cÃ©dent](02-jvm-platform-threads.md) | [â¡ï¸ Suivant: Introduction aux Virtual Threads](04-virtual-threads-intro.md)